<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>outlier_detection | GeneWlan</title><meta name="author" content="ZhangLei"><meta name="copyright" content="ZhangLei"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="[TOC] 单变量和多变量异常值 异常值是偏离数据集中大多数样本点的数据点。出现异常值的原因有很多，例如自然偏差、欺诈活动、人为或系统错误。不过，在我们进行任何统计分析或训练机器学习模型之前，对数据检测和识别异常值都是必不可少的，这个预处理的过程会影响最后的效果。 本文覆盖“单变量”和“多变量”异常值场景、以及使用统计方法和机器学习异常检测技术来识别它们，包括四分位距和标准差方法、孤立森林、DBS">
<meta property="og:type" content="article">
<meta property="og:title" content="outlier_detection">
<meta property="og:url" content="http://genewlan.github.io/2023/07/31/outlier-detection/index.html">
<meta property="og:site_name" content="GeneWlan">
<meta property="og:description" content="[TOC] 单变量和多变量异常值 异常值是偏离数据集中大多数样本点的数据点。出现异常值的原因有很多，例如自然偏差、欺诈活动、人为或系统错误。不过，在我们进行任何统计分析或训练机器学习模型之前，对数据检测和识别异常值都是必不可少的，这个预处理的过程会影响最后的效果。 本文覆盖“单变量”和“多变量”异常值场景、以及使用统计方法和机器学习异常检测技术来识别它们，包括四分位距和标准差方法、孤立森林、DBS">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2023-07-31T14:40:34.000Z">
<meta property="article:modified_time" content="2023-08-31T07:43:31.154Z">
<meta property="article:author" content="ZhangLei">
<meta property="article:tag" content="Modeling">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://genewlan.github.io/2023/07/31/outlier-detection/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'outlier_detection',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-08-31 15:43:31'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
    <style>
      #background-effect {
        position: fixed !important;
        top: 0px;
        left: 0px;
        z-index: -1;
        width: 100%;
        height: 100%
      }
    </style>
  <!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="GeneWlan" type="application/atom+xml">
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">18</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2023/03/21/ZVUmQFknE2JosXT.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="GeneWlan"><span class="site-name">GeneWlan</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">outlier_detection</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-07-31T14:40:34.000Z" title="发表于 2023-07-31 22:40:34">2023-07-31</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-08-31T07:43:31.154Z" title="更新于 2023-08-31 15:43:31">2023-08-31</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="outlier_detection"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>[TOC]</p>
<h1>单变量和多变量异常值</h1>
<p><strong>异常值</strong>是偏离数据集中大多数样本点的数据点。出现异常值的原因有很多，例如自然偏差、欺诈活动、人为或系统错误。不过，在我们进行任何统计分析或训练机器学习模型之前，对数据检测和识别异常值都是必不可少的，这个预处理的过程会影响最后的效果。</p>
<p>本文覆盖“单变量”和“多变量”异常值场景、以及使用统计方法和机器学习异常检测技术来识别它们，包括四分位距和标准差方法、孤立森林、DBSCAN模型以及 LOF 局部离群因子模型等。</p>
<p>通过使用<code>seaborn</code>的<code>pairplot</code>我们可以绘制数据集不同字段之间的两两分布关系，可以<strong>可视化</strong>地查看数据的分布情况。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#两两分布关系</span><br><span class="line">import pandas as pd</span><br><span class="line">glass = pd.read_csv(&#x27;glass.csv&#x27;)</span><br><span class="line">import seaborn as sns</span><br><span class="line">sns.pairplot(glass, diag_kws=&#123;&#x27;color&#x27;:&#x27;red&#x27;&#125;)</span><br></pre></td></tr></table></figure>
<p>pairplot 的结果包含两两数据的关联分析和每个变量的分布结果，其中对角线为单变量的分布可视化，我们发现并非所有属性字段都具有遵循<strong>正态分布</strong>。<strong>事实上，大多数属性都偏向较低值（即 Ba、Fe*）或较高值（即 Mg）</strong></p>
<p>如果要检测<strong>单变量异常值</strong>，我们应该关注单个属性的分布，并找到远离该属性大部分数据的数据点。例如，如果我们选择属性“Na”并绘制箱线图，可以找到哪些数据点在上下边界之外，可以标记为异常值。</p>
<p>如果要检测<strong>多变量异常值</strong>，我们应该关注 n 维空间中至少两个变量的组合。例如，在上述数据集中，我们可以使用玻璃的所有八个属性并将它们绘制在 n 维空间中，并通过检测哪些数据点落在远处来找到多元异常值。</p>
<p>但是因为绘制三维以上的图非常困难，我们要想办法将八个维度的数据在低维空间内表征。我们可以使用PCA（主成分分析）降维方法完成，具体的代码如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.decomposition import PCA</span><br><span class="line">import plotly.express as px</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"># Dimensionality reduction to 3 dimensions</span><br><span class="line">pca = PCA(n_components=3) </span><br><span class="line">glass_pca = pca.fit_transform(glass.iloc[:, :-1])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"># 3D scatterplot</span><br><span class="line">fig = px.scatter_3d(x=glass_pca[:, 0],</span><br><span class="line">                    y=glass_pca[:, 1], </span><br><span class="line">                    z=glass_pca[:, 2],</span><br><span class="line">                    color=glass.iloc[:, -1])</span><br><span class="line">fig.show()</span><br></pre></td></tr></table></figure>
<p>在上图中可以看到，有些数据点彼此靠近（组成密集区域），有些距离很远，可能是<strong>多变量异常值</strong>。</p>
<h1>单变量异常值检测</h1>
<p><img src="/2023/07/31/outlier-detection/resize4.png" alt></p>
<h2 id="基于分布-标准差法">基于分布–标准差法</h2>
<p>假设一个变量是正态分布的，那它的直方图应遵循正态分布曲线（如下图所示），其中 68.2% 的数据值位于距均值1个标准差范围内，95.4% 的数据值位于距均值2个标准差范围内，99.7% 的数据值位于距均值3个标准差范围内。因此，如果有数据点距离平均值超过3个标准差，我们就可以将其视作异常值。这也是著名的异常检测3sigma法。具体的的代码实现如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Find mean, standard deviation and cut off value </span><br><span class="line">mean = glass[&quot;Na&quot;].mean()</span><br><span class="line">std = glass[&quot;Na&quot;].std()</span><br><span class="line">cutoff = 3 * std</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"># Define lower and upper boundaries</span><br><span class="line">lower, upper = mean-cutoff, mean+cutoff</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"># Define new dataset by masking upper and lower boundaries</span><br><span class="line">new_glass = glass[(glass[&quot;Na&quot;] &gt; lower) &amp; (glass[&quot;Na&quot;] &lt; upper)]</span><br></pre></td></tr></table></figure>
<p>通过使用标准偏差法，我们基于“Na”变量删除了2条极端记录。大家可以用同样的方法在其他属性上，检测和移除单变量异常值</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Shape of original dataset: (213, 9)</span><br><span class="line">Shape of dataset after removing outliers in Na column: (211, 9)</span><br></pre></td></tr></table></figure>
<h2 id="基于分布-四分位距法">基于分布–四分位距法</h2>
<p>四分位数间距方法是一个基于箱线图的统计方法，它通过定义三个数据分布位点将数据进行划分，并计算得到统计边界值：</p>
<ul>
<li>四分位数 1 (Q1) 表示第 25 个百分位数</li>
<li>四分位数 2 (Q2) 表示第 50 个百分位数</li>
<li>四分位数 3 (Q3) 表示第 75 个百分位数</li>
</ul>
<p>箱线图中的方框表示 IQR 范围，定义为 Q1 和 Q3 之间的范围：<code>IQR = Q3 — Q1</code></p>
<p>低于的数据点<code>Q1 - 1.5*IQR</code>或以上<code>Q3 + 1.5*IQR</code>被定义为异常值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Find Q1, Q3, IQR and cut off value </span><br><span class="line">q25, q75 = np.quantile(glass[&quot;Na&quot;], 0.25), np.quantile(glass[&quot;Na&quot;], 0.75)</span><br><span class="line">iqr = q75 - q25</span><br><span class="line">cutoff = 1.5 * iqr</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"># Define lower and upper boundaries</span><br><span class="line">lower, upper = q25 - cutoff, q75 + cutoff</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"># Define new dataset by masking upper and lower boundaries</span><br><span class="line">new_glass = glass[(glass[&quot;Na&quot;] &gt; lower) &amp; (glass[&quot;Na&quot;] &lt; upper)]</span><br><span class="line">Shape of original dataset: (213, 9)</span><br><span class="line">Shape of dataset after removing outliers in Na column: (206, 9)</span><br></pre></td></tr></table></figure>
<p>我们可以看到，基于 IQR 技术，从“Na”变量维度我们删除了七个记录。我们注意到，基于标准偏差方法只能找到 2 个异常值，是非常极端的极值点，但是使用 IQR 方法我们能够检测到更多（5 个不是那么极端的记录）。</p>
<h2 id="基于分布-z-score">基于分布–Z-score</h2>
<p>Z-score为标准分数，测量数据点和平均值的距离，若A与平均值相差2个标准差，Z-score为2。当把Z-score=3作为阈值去剔除异常点时，便相当于3sigma。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def z_score(s):</span><br><span class="line">  z_score = (s - np.mean(s)) / np.std(s)</span><br><span class="line">  return z_score</span><br></pre></td></tr></table></figure>
<h2 id="基于分布-grubbs假设检验">基于分布–Grubbs假设检验</h2>
<p>Grubbs’Test为一种假设检验的方法，常被用来检验服从正态分布的单变量数据集（univariate data set）Y中的单个异常值。若有异常值，则其必为数据集中的最大值或最小值。原假设与备择假设如下：</p>
<p>H0: 数据集中没有异常值<br>
H1: 数据集中有一个异常值<br>
使用Grubbs测试需要总体是正态分布的。算法流程：</p>
<ol>
<li>样本从小到大排序</li>
<li>求样本的mean和dev</li>
<li>计算min/max与mean的差距，更大的那个为可疑值</li>
<li>求可疑值的z-score (standard score)，如果大于Grubbs临界值，那么就是outlier</li>
</ol>
<p>Grubbs临界值可以查表得到，它由两个值决定：检出水平α（越严格越小），样本数量n，排除outlier，对剩余序列循环做 1-4 步骤 [1]。详细计算样例可以参考。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from outliers import smirnov_grubbs as grubbs</span><br><span class="line">print(grubbs.test([8, 9, 10, 1, 9], alpha=0.05))</span><br><span class="line">print(grubbs.min_test_outliers([8, 9, 10, 1, 9], alpha=0.05))</span><br><span class="line">print(grubbs.max_test_outliers([8, 9, 10, 1, 9], alpha=0.05))</span><br><span class="line">print(grubbs.max_test_indices([8, 9, 10, 50, 9], alpha=0.05))</span><br></pre></td></tr></table></figure>
<h2 id="基于距离的方法-knn">基于距离的方法-KNN</h2>
<p>依次计算每个样本点与它最近的K个样本的平均距离，再利用计算的距离与阈值进行比较，如果大于阈值，则认为是异常点。优点是不需要假设数据的分布，缺点是仅可以找出全局异常点，无法找到局部异常点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from pyod.models.knn import KNN</span><br><span class="line"></span><br><span class="line"># 初始化检测器clf</span><br><span class="line">clf = KNN( method=&#x27;mean&#x27;, n_neighbors=3, )</span><br><span class="line">clf.fit(X_train)</span><br><span class="line"># 返回训练数据上的分类标签 (0: 正常值, 1: 异常值)</span><br><span class="line">y_train_pred = clf.labels_</span><br><span class="line"># 返回训练数据上的异常值 (分值越大越异常)</span><br><span class="line">y_train_scores = clf.decision_scores_</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1>多变量异常值检测</h1>
<h2 id="孤立森林算法-isolation-forest">孤立森林算法-Isolation Forest</h2>
<p><strong>孤立森林</strong>是一种基于随机森林的无监督机器学习算法。我们都知道，随机森林是一种集成学习模型，它使用基模型（比如 100 个决策树）组合和集成完成最后的预估。孤立森林遵循随机森林的方法，但相比之下，它检测（或叫做隔离）异常数据点。它有两个基本假设：离群值是少数样本，且它们是分布偏离的。</p>
<p><strong>孤立森林通过随机选择一个特征，然后随机选择一个分割规则来分割所选特征的值来创建决策树</strong>。这个过程一直持续到达到设置的超参数值。在构建好的孤立森林中，如果树更短且对应分支样本数更少，则相应的值是异常值（少数和不寻常）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import IsolationForest</span><br><span class="line">IsolationForest(n_estimators=100, max_samples=&#x27;auto&#x27;, contamination=&#x27;auto&#x27;, max_features=1.0, bootstrap=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)</span><br><span class="line">n_estimators：表示要集成的基模型的数量。</span><br><span class="line">max_samples：表示用于训练模型的样本数。</span><br><span class="line">contamination：用于定义数据中异常值的比例。</span><br><span class="line">max_features：表示采样处的用于训练的特征数。</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import IsolationForest</span><br><span class="line"> </span><br><span class="line"># Initiate isolation forest</span><br><span class="line">isolation = IsolationForest(n_estimators=100, </span><br><span class="line">                            contamination=&#x27;auto&#x27;, </span><br><span class="line">                            max_features=glass.shape[1]) </span><br><span class="line"> </span><br><span class="line"># Fit and predict</span><br><span class="line">isolation.fit(glass)</span><br><span class="line">outliers_predicted = isolation.predict(glass)</span><br><span class="line"> </span><br><span class="line"># Address outliers in a new column</span><br><span class="line">glass[&#x27;outlier&#x27;] = outliers_predicted</span><br></pre></td></tr></table></figure>
<p>我们通过将基模型的数量设置为 100，将最大特征设置为特征总数，将异常值占比设置为<code>'auto'</code>，如果把它为 0.1，则总体 10% 的数据集将被定义为异常值。</p>
<p>我们在使用孤立森林学习后，调用 <code>glass['outlier'].value_counts()</code>可以看到有 19 条记录被标记为<code>-1</code>（即异常值），其余 195 条记录被标记为<code>1</code>（正常值）。</p>
<h2 id="基于空间密度的聚类算法-dbscan">基于空间密度的聚类算法-DBSCAN</h2>
<p><strong>DBSCAN</strong> 是一种流行的聚类算法，通常用作 K-means 的替代方法。它是基于分布密度的，专注于许多数据点所在的高密度区域。它通过测量数据之间的特征空间距离（即欧氏距离）来识别哪些样本可以聚类在一起。DBSCAN 作为聚类算法最大的优势之一就是我们不需要预先定义聚类的数量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import DBSCAN</span><br><span class="line">DBSCAN(eps=0.5, min_samples=5, metric=&#x27;euclidean&#x27;, metric_params=None, algorithm=&#x27;auto&#x27;, leaf_size=30, p=None, n_jobs=None)</span><br><span class="line">eps（epsilon）：考虑在同一个 cluster 中的两个数据点之间的最大距离。</span><br><span class="line">min_samples：核心点的接近数据点的数量。</span><br><span class="line">metric：用于计算不同点之间的距离度量方法。</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.cluster import DBSCAN</span><br><span class="line">from sklearn.preprocessing import MinMaxScaler</span><br><span class="line">  </span><br><span class="line"># Initiate DBSCAN</span><br><span class="line">dbscan = DBSCAN(eps=0.4, min_samples=10)</span><br><span class="line"> </span><br><span class="line"># Transform data</span><br><span class="line">glass_x = np.array(glass).astype(&#x27;float&#x27;)</span><br><span class="line"> </span><br><span class="line"># Initiate scaler and scale data</span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">glass_scaled = scaler.fit_transform(glass_x)</span><br><span class="line">  </span><br><span class="line"># Fit DBSCAN on scaled data</span><br><span class="line">dbscan.fit(glass_scaled)</span><br><span class="line">  </span><br><span class="line"># Address outliers in a new column</span><br><span class="line">glass[&#x27;outlier&#x27;] = dbscan.labels_</span><br></pre></td></tr></table></figure>
<p>在启动 DBSCAN 时，仔细选择超参数非常重要。例如，如果 eps 值选择得太小，那么大部分数据都可以归类为离群值，因为邻域区域被定义为更小。相反，如果 eps 值选择太大，则大多数点会被聚类算法聚到一起，因为它们很可能位于同一邻域内。这里我们使用 k 距离图选择 eps 为 0.4。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math </span><br><span class="line"> </span><br><span class="line"><span class="comment"># Function to calculate k distance</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_k_distance</span>(<span class="params">X,k</span>):</span><br><span class="line">  </span><br><span class="line">    k_distance = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X)):</span><br><span class="line">        euclidean_dist = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X)):</span><br><span class="line">            euclidean_dist.append(</span><br><span class="line">                math.sqrt(</span><br><span class="line">                    ((X[i,<span class="number">0</span>] - X[j,<span class="number">0</span>]) ** <span class="number">2</span>) +</span><br><span class="line">                    ((X[i,<span class="number">1</span>] - X[j,<span class="number">1</span>]) ** <span class="number">2</span>)))</span><br><span class="line">  </span><br><span class="line">        euclidean_dist.sort()</span><br><span class="line">        k_distance.append(euclidean_dist[k])</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> k_distance</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Calculate and plot epsilon distance</span></span><br><span class="line">eps_distance = calculate_k_distance(glass_scaled, <span class="number">10</span>)</span><br><span class="line">px.histogram(eps_distance, labels=&#123;<span class="string">&#x27;value&#x27;</span>:<span class="string">&#x27;Epsilon distance&#x27;</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>此外，<code>min_samples</code>是一个重要的超参数，通常等于或大于 3，大多数情况下选择 D+1，其中 D 是数据集的维度。在上述代码中，我们将<code>min_samples</code>设置为 10。</p>
<p>由于 DBSCAN 是通过密度来识别簇的，所以高密度区域是簇出现的地方，低密度区域是异常值出现的地方。经过DBSCAN建模，我们调用<code>glass['outlier'].value_counts()</code>可以看到有 22 条记录被标记为<code>-1</code>（异常值），其余 192 条记录被标记为<code>1</code>（正常值）。</p>
<h2 id="基于密度的方法-局部异常因子算法-lof">基于密度的方法–局部异常因子算法-LOF</h2>
<p>LOF是基于密度的经典算法（Breuning et. al. 2000），通过给每个数据点都分配一个依赖于邻域密度的离群因子 LOF，进而判断该数据点是否为离群点。它的好处在于可以量化每个数据点的异常程度（outlierness）。<strong>LOF</strong>是一种流行的无监督异常检测算法，它计算数据点相对于其邻居的<strong>局部密度偏差</strong>。计算完成后，密度较低的点被视为异常值。</p>
<p>数据点 的局部相对密度（局部异常因子）为点 邻域内点的平均局部可达密度跟数据 点 的局部可达密度的比值, 即：</p>
<p>数据点P的局部可达密度=P最近邻的平均可达距离的倒数。距离越大，密度越小。</p>
<p>点P到点O的第k可达距离=max(点O的k近邻距离，点P到点O的距离)。</p>
<p>点O的k近邻距离=第k个最近的点跟点O之间的距离。</p>
<p>整体来说，LOF算法流程如下：</p>
<p>对于每个数据点，计算它与其他所有点的距离，并按从近到远排序；<br>
对于每个数据点，找到它的K-Nearest-Neighbor，计算LOF得分。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neighbors import LocalOutlierFactor</span><br><span class="line">LocalOutlierFactor(n_neighbors=20, algorithm=&#x27;auto&#x27;, leaf_size=30, metric=&#x27;minkowski&#x27;, p=2, metric_params=None, contamination=&#x27;auto&#x27;, novelty=False, n_jobs=None)</span><br><span class="line">n_neighbors：用于选择默认等于 20 的邻居数量。</span><br><span class="line">contamination：用于定义离群值比例。</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> LocalOutlierFactor</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Initiate LOF</span></span><br><span class="line">lof = LocalOutlierFactor(n_neighbors=<span class="number">20</span>, contamination=<span class="string">&#x27;auto&#x27;</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Transform data</span></span><br><span class="line">glass_x = np.array(glass).astype(<span class="string">&#x27;float&#x27;</span>) </span><br><span class="line"> </span><br><span class="line"><span class="comment"># Initiate scaler and scale data</span></span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">glass_scaled = scaler.fit_transform(glass_x)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Fit and predict on scaled data</span></span><br><span class="line">clf = LocalOutlierFactor()</span><br><span class="line">outliers_predicted = clf.fit_predict(glass) </span><br><span class="line"> </span><br><span class="line"><span class="comment"># Address outliers in a new column</span></span><br><span class="line">glass[<span class="string">&#x27;outlier&#x27;</span>] = outliers_predicted</span><br></pre></td></tr></table></figure>
<h2 id="基于密度的方法-connectivity-based-outlier-factor-cof">基于密度的方法–Connectivity-Based Outlier Factor (COF)</h2>
<p>COF是LOF的变种，相比于LOF，COF可以处理低密度下的异常值，COF的局部密度是基于平均链式距离计算得到。在一开始的时候我们一样会先计算出每个点的k-nearest neighbor。而接下来我们会计算每个点的Set based nearest Path</p>
<p>假使我们今天我们的k=5，所以F的neighbor为B、C、D、E、G。而对于F离他最近的点为E，所以SBN Path的第一个元素是F、第二个是E。离E最近的点为D所以第三个元素为D，接下来离D最近的点为C和G，所以第四和五个元素为C和G，最后离C最近的点为B，第六个元素为B。所以整个流程下来，F的SBN Path为{F, E, D, C, G, C, B}。而对于SBN Path所对应的距离e={e1, e2, e3,…,ek}，依照上面的例子e={3,2,1,1,1}。</p>
<p>所以我们可以说假使我们想计算p点的SBN Path，我们只要直接计算p点和其neighbor所有点所构成的graph的minimum spanning tree，之后我们再以p点为起点执行shortest path算法，就可以得到我们的SBN Path。</p>
<p>而接下来我们有了SBN Path我们就会接着计算，p点的链式距离，有了ac_distance后，我们就可以计算COF：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from pyod.models.cof import COF</span><br><span class="line">cof = COF(contamination = 0.06,  ## 异常值所占的比例</span><br><span class="line">          n_neighbors = 20,      ## 近邻数量</span><br><span class="line">        )</span><br><span class="line">cof_label = cof.fit_predict(iris.values) # 鸢尾花数据</span><br><span class="line">print(&quot;检测出的异常值数量为:&quot;,np.sum(cof_label == 1))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="基于密度的方法-stochastic-outlier-selection-sos">基于密度的方法–Stochastic Outlier Selection (SOS)</h2>
<p>将特征矩阵（feature martrix）或者相异度矩阵（dissimilarity matrix）输入给SOS算法，会返回一个异常概率值向量（每个点对应一个）。SOS的思想是：当一个点和其它所有点的关联度（affinity）都很小的时候，它就是一个异常点。</p>
<p>SOS的流程：计算相异度矩阵D；计算关联度矩阵A；计算关联概率矩阵B；算出异常概率向量。</p>
<p>相异度矩阵D是各样本两两之间的度量距离, 比如欧式距离或汉明距离等。关联度矩阵反映的是 度量距离方差, 如图7, 点 的密度最大, 方差最小; 的密度最小, 方差最大。而关联概率 矩阵 (binding probability matrix)就是把关联矩阵(affinity matrix)按行归一化得到的</p>
<p><img src="/2023/07/31/outlier-detection/resize.png" alt></p>
<p>得到了binding probability matrix，每个点的异常概率值就用如下的公式计算，当一个点和其它所有点的关联度（affinity）都很小的时候，它就是一个异常点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">from sksos import SOS</span><br><span class="line">iris = pd.read_csv(&quot;http://bit.ly/iris-csv&quot;)</span><br><span class="line">X = iris.drop(&quot;Name&quot;, axis=1).values</span><br><span class="line">detector = SOS()</span><br><span class="line">iris[&quot;score&quot;] = detector.predict(X)</span><br><span class="line">iris.sort_values(&quot;score&quot;, ascending=False).head(10)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="基于分类的方法-oneclasssvm">基于分类的方法–OneClassSVM</h2>
<p>OneClassSVM是一种用于检测异常点的算法，是一种无监督学习算法。决策边界将数据点分为两类：内点和外点。非离群点是与训练集中的大多数点相似的点，而离群点是与训练集中的大多数点显着不同的点。</p>
<p>为了学习决策边界，OneClassSVM最大化边界和内点之间的距离，最终找到合适的超平面。这个超平面可以最大化内点和决策边界之间的边距。一旦学习了决策边界，就可以使用它来将新点分类为内点或异常点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.svm import OneClassSVM</span><br><span class="line">X = [[0], [0.44], [0.45], [0.46], [1]]</span><br><span class="line">clf = OneClassSVM(gamma=&#x27;auto&#x27;).fit(X)</span><br><span class="line"></span><br><span class="line"># 异常/离群值返回 -1，离群值返回 +1</span><br><span class="line">clf.predict(X)</span><br><span class="line">kernel：SVM内核类型</span><br><span class="line">nu：训练误差分数的上限</span><br></pre></td></tr></table></figure>
<p>在实际使用中OneClassSVM速度较慢，因此可以考虑使用随机梯度下降求解线性的SVM来代替，也就是SGDOneClassSVM。</p>
<h2 id="基于分类的方法-elliptic-envelope">基于分类的方法–Elliptic Envelope</h2>
<p>椭圆包络（Elliptic Envelope）是一种检测数据集中异常或异常数据点的方法。它是一种无监督学习方法，通过将椭圆拟合到训练集中的数据点来工作，但假设大多数点遵循高斯分布。</p>
<p>为了拟合椭圆，椭圆包络估计数据点的均值和协方差，并使用这些估计值来确定椭圆的形状和方向。一旦学习了椭圆，它就可以用来将新点分类为内点或异常点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.covariance import EllipticEnvelope</span><br><span class="line">true_cov = np.array([[.8, .3],</span><br><span class="line">                     [.3, .4]])</span><br><span class="line">X = np.random.RandomState(0).multivariate_normal(mean=[0, 0],</span><br><span class="line">                                                 cov=true_cov,</span><br><span class="line">                                                 size=500)</span><br><span class="line">cov = EllipticEnvelope(random_state=0).fit(X)</span><br><span class="line"># predict returns 1 for an inlier and -1 for an outlier</span><br><span class="line">cov.predict([[0, 0],</span><br><span class="line">             [3, 3]])</span><br></pre></td></tr></table></figure>
<h2 id="基于降维的方法-principal-component-analysis-pca">基于降维的方法–Principal Component Analysis (PCA)</h2>
<p>PCA在异常检测方面的做法，大体有两种思路：</p>
<p>(1) 将数据映射到低维特征空间，然后在特征空间不同维度上查看每个数据点跟其它数据的偏差；</p>
<p>(2) 将数据映射到低维特征空间，然后由低维特征空间重新映射回原空间，尝试用低维特征重构原始数据，看重构误差的大小。</p>
<p>PCA在做特征值分解，会得到：</p>
<p>特征向量：反应了原始数据方差变化程度的不同方向；<br>
特征值：数据在对应方向上的方差大小。</p>
<p>所以，最大特征值对应的特征向量为数据方差最大的方向，最小特征值对应的特征向量为数据方差最小的方向。原始数据在不同方向上的方差变化反应了其内在特点。如果单个数据样本跟整体数据样本表现出的特点不太一致，比高。 是特征值, 用于归一化, 使不同方向上的偏离程度具有可比性。</p>
<p>在计算异常分数时，关于特征向量（即度量异常用的标杆）选择又有两种方式：</p>
<p>考虑在前k个特征向量方向上的偏差：前k个特征向量往往直接对应原始数据里的某几个特征，在前几个特征向量方向上偏差比较大的数据样本，往往就是在原始数据中那几个特征上的极值点。<br>
考虑后r个特征向量方向上的偏差：后r个特征向量通常表示某几个原始特征的线性组合，线性组合之后的方差比较小反应了这几个特征之间的某种关系。在后几个特征方向上偏差比较大的数据样本，表示它在原始数据里对应的那几个特征上出现了与预计不太一致的情况。<br>
得分大于阈值C则判断为异常。</p>
<p>第二种做法，PCA提取了数据的主要特征，如果一个数据样本不容易被重构出来，表示这个数据样本的特征跟整体数据样本的特征不一致，那么它显然就是一个异常的样本：</p>
<p>其中, 是基于 维特征向量重构的样本。</p>
<p>基于低维特征进行数据样本的重构时，舍弃了较小的特征值对应的特征向量方向上的信息。换一句话说，重构误差其实主要来自较小的特征值对应的特征向量方向上的信息。基于这个直观的理解，PCA在异常检测上的两种不同思路都会特别关注较小的特征值对应的特征向量。所以，我们说PCA在做异常检测时候的两种思路本质上是相似的，当然第一种方法还可以关注较大特征值对应的特征向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA()</span><br><span class="line">pca.fit(centered_training_data)</span><br><span class="line">transformed_data = pca.transform(training_data)</span><br><span class="line">y = transformed_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算异常分数</span></span><br><span class="line">lambdas = pca.singular_values_</span><br><span class="line">M = ((y*y)/lambdas)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前k个特征向量和后r个特征向量</span></span><br><span class="line">q = <span class="number">5</span></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;Explained variance by first q terms: &quot;</span>, <span class="built_in">sum</span>(pca.explained_variance_ratio_[:q])</span><br><span class="line">q_values = <span class="built_in">list</span>(pca.singular_values_ &lt; <span class="number">.2</span>)</span><br><span class="line">r = q_values.index(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对每个样本点进行距离求和的计算</span></span><br><span class="line">major_components = M[:,<span class="built_in">range</span>(q)]</span><br><span class="line">minor_components = M[:,<span class="built_in">range</span>(r, <span class="built_in">len</span>(features))]</span><br><span class="line">major_components = np.<span class="built_in">sum</span>(major_components, axis=<span class="number">1</span>)</span><br><span class="line">minor_components = np.<span class="built_in">sum</span>(minor_components, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 人为设定c1、c2阈值</span></span><br><span class="line">components = pd.DataFrame(&#123;<span class="string">&#x27;major_components&#x27;</span>: major_components, </span><br><span class="line">                               <span class="string">&#x27;minor_components&#x27;</span>: minor_components&#125;)</span><br><span class="line">c1 = components.quantile(<span class="number">0.99</span>)[<span class="string">&#x27;major_components&#x27;</span>]</span><br><span class="line">c2 = components.quantile(<span class="number">0.99</span>)[<span class="string">&#x27;minor_components&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 制作分类器</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">classifier</span>(<span class="params">major_components, minor_components</span>):  </span><br><span class="line">    major = major_components &gt; c1</span><br><span class="line">    minor = minor_components &gt; c2    </span><br><span class="line">    <span class="keyword">return</span> np.logical_or(major,minor)</span><br><span class="line"></span><br><span class="line">results = classifier(major_components=major_components, minor_components=minor_components)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="基于降维的方法-autoencoder">基于降维的方法–AutoEncoder</h2>
<p>PCA是线性降维，AutoEncoder是非线性降维。根据正常数据训练出来的AutoEncoder，能够将正常样本重建还原，但是却无法将异于正常分布的数据点较好地还原，导致还原误差较大。因此如果一个新样本被编码，解码之后，它的误差超出正常数据编码和解码后的误差范围，则视作为异常数据。需要注意的是，AutoEncoder训练使用的数据是正常数据（即无异常值），这样才能得到重构后误差分布范围是多少以内是合理正常的。所以AutoEncoder在这里做异常检测时，算是一种有监督学习的方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Ref: [https://zhuanlan.zhihu.com/p/260882741](https://zhuanlan.zhihu.com/p/260882741)</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化数据</span></span><br><span class="line">scaler = preprocessing.MinMaxScaler()</span><br><span class="line">X_train = pd.DataFrame(scaler.fit_transform(dataset_train),</span><br><span class="line">                              columns=dataset_train.columns,</span><br><span class="line">                              index=dataset_train.index)</span><br><span class="line"><span class="comment"># Random shuffle training data</span></span><br><span class="line">X_train.sample(frac=<span class="number">1</span>)</span><br><span class="line">X_test = pd.DataFrame(scaler.transform(dataset_test),</span><br><span class="line">                             columns=dataset_test.columns,</span><br><span class="line">                             index=dataset_test.index)</span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">10</span>)</span><br><span class="line">act_func = <span class="string">&#x27;relu&#x27;</span></span><br><span class="line"><span class="comment"># Input layer:</span></span><br><span class="line">model=Sequential()</span><br><span class="line"><span class="comment"># First hidden layer, connected to input vector X.</span></span><br><span class="line">model.add(Dense(<span class="number">10</span>,activation=act_func,</span><br><span class="line">                kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">                kernel_regularizer=regularizers.l2(<span class="number">0.0</span>),</span><br><span class="line">                input_shape=(X_train.shape[<span class="number">1</span>],)</span><br><span class="line">               )</span><br><span class="line">         )</span><br><span class="line">model.add(Dense(<span class="number">2</span>,activation=act_func,</span><br><span class="line">                kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>))</span><br><span class="line">model.add(Dense(<span class="number">10</span>,activation=act_func,</span><br><span class="line">                kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>))</span><br><span class="line">model.add(Dense(X_train.shape[<span class="number">1</span>],</span><br><span class="line">                kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>))</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mse&#x27;</span>,optimizer=<span class="string">&#x27;adam&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(model.summary())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train model for 100 epochs, batch size of 10:</span></span><br><span class="line">NUM_EPOCHS=<span class="number">100</span></span><br><span class="line">BATCH_SIZE=<span class="number">10</span></span><br><span class="line">history=model.fit(np.array(X_train),np.array(X_train),</span><br><span class="line">                  batch_size=BATCH_SIZE,</span><br><span class="line">                  epochs=NUM_EPOCHS,</span><br><span class="line">                  validation_split=<span class="number">0.05</span>,</span><br><span class="line">                  verbose = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;loss&#x27;</span>],</span><br><span class="line">         <span class="string">&#x27;b&#x27;</span>,</span><br><span class="line">         label=<span class="string">&#x27;Training loss&#x27;</span>)</span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;val_loss&#x27;</span>],</span><br><span class="line">         <span class="string">&#x27;r&#x27;</span>,</span><br><span class="line">         label=<span class="string">&#x27;Validation loss&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss, [mse]&#x27;</span>)</span><br><span class="line">plt.ylim([<span class="number">0</span>,<span class="number">.1</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看训练集还原的误差分布如何，以便制定正常的误差分布范围</span></span><br><span class="line">X_pred = model.predict(np.array(X_train))</span><br><span class="line">X_pred = pd.DataFrame(X_pred,</span><br><span class="line">                      columns=X_train.columns)</span><br><span class="line">X_pred.index = X_train.index</span><br><span class="line"></span><br><span class="line">scored = pd.DataFrame(index=X_train.index)</span><br><span class="line">scored[<span class="string">&#x27;Loss_mae&#x27;</span>] = np.mean(np.<span class="built_in">abs</span>(X_pred-X_train), axis = <span class="number">1</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">sns.distplot(scored[<span class="string">&#x27;Loss_mae&#x27;</span>],</span><br><span class="line">             bins = <span class="number">10</span>,</span><br><span class="line">             kde= <span class="literal">True</span>,</span><br><span class="line">            color = <span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">plt.xlim([<span class="number">0.0</span>,<span class="number">.5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 误差阈值比对，找出异常值</span></span><br><span class="line">X_pred = model.predict(np.array(X_test))</span><br><span class="line">X_pred = pd.DataFrame(X_pred,</span><br><span class="line">                      columns=X_test.columns)</span><br><span class="line">X_pred.index = X_test.index</span><br><span class="line">threshod = <span class="number">0.3</span></span><br><span class="line">scored = pd.DataFrame(index=X_test.index)</span><br><span class="line">scored[<span class="string">&#x27;Loss_mae&#x27;</span>] = np.mean(np.<span class="built_in">abs</span>(X_pred-X_test), axis = <span class="number">1</span>)</span><br><span class="line">scored[<span class="string">&#x27;Threshold&#x27;</span>] = threshod</span><br><span class="line">scored[<span class="string">&#x27;Anomaly&#x27;</span>] = scored[<span class="string">&#x27;Loss_mae&#x27;</span>] &gt; scored[<span class="string">&#x27;Threshold&#x27;</span>]</span><br><span class="line">scored.head()</span><br></pre></td></tr></table></figure>
<h1>离群检测与新奇检测</h1>
<p>离群检测（Outlier detection）：训练数据包含离群值，这些离群值被定义为与其他观察值相差甚远的观察值。<br>
新奇检测 (Novelty detection)：训练数据没有离群点，需要观察新的样本是否包含离群点。<br>
离群检测和新颖性检测都用于异常检测，其中人们对检测异常或不寻常的观察感兴趣。离群检测也称为无监督异常检测，新奇检测称为半监督异常检测。</p>
<p>在离群检测中离群值不能形成密集的集群，因为可以假设离群值位于低密度区域。相反在新颖性检测中，新颖性处于训练数据的低密度区域。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">​</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">​</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons, make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.covariance <span class="keyword">import</span> EllipticEnvelope</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> IsolationForest</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> LocalOutlierFactor</span><br><span class="line">​</span><br><span class="line"><span class="built_in">print</span>(__doc__)</span><br><span class="line">​</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;contour.negative_linestyle&#x27;</span>] = <span class="string">&#x27;solid&#x27;</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># Example settings</span></span><br><span class="line">n_samples = <span class="number">300</span></span><br><span class="line">outliers_fraction = <span class="number">0.15</span></span><br><span class="line">n_outliers = <span class="built_in">int</span>(outliers_fraction * n_samples)</span><br><span class="line">n_inliers = n_samples - n_outliers</span><br><span class="line">​</span><br><span class="line"><span class="comment"># define outlier/anomaly detection methods to be compared</span></span><br><span class="line">anomaly_algorithms = [</span><br><span class="line">    (<span class="string">&quot;Robust covariance&quot;</span>, EllipticEnvelope(contamination=outliers_fraction)),</span><br><span class="line">    (<span class="string">&quot;One-Class SVM&quot;</span>, svm.OneClassSVM(nu=outliers_fraction, kernel=<span class="string">&quot;rbf&quot;</span>,</span><br><span class="line">                                      gamma=<span class="number">0.1</span>)),</span><br><span class="line">    (<span class="string">&quot;Isolation Forest&quot;</span>, IsolationForest(contamination=outliers_fraction,</span><br><span class="line">                                         random_state=<span class="number">42</span>)),</span><br><span class="line">    (<span class="string">&quot;Local Outlier Factor&quot;</span>, LocalOutlierFactor(</span><br><span class="line">        n_neighbors=<span class="number">35</span>, contamination=outliers_fraction))]</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Define datasets</span></span><br><span class="line">blobs_params = <span class="built_in">dict</span>(random_state=<span class="number">0</span>, n_samples=n_inliers, n_features=<span class="number">2</span>)</span><br><span class="line">datasets = [</span><br><span class="line">    make_blobs(centers=[[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>]], cluster_std=<span class="number">0.5</span>,</span><br><span class="line">               **blobs_params)[<span class="number">0</span>],</span><br><span class="line">    make_blobs(centers=[[<span class="number">2</span>, <span class="number">2</span>], [-<span class="number">2</span>, -<span class="number">2</span>]], cluster_std=[<span class="number">0.5</span>, <span class="number">0.5</span>],</span><br><span class="line">               **blobs_params)[<span class="number">0</span>],</span><br><span class="line">    make_blobs(centers=[[<span class="number">2</span>, <span class="number">2</span>], [-<span class="number">2</span>, -<span class="number">2</span>]], cluster_std=[<span class="number">1.5</span>, <span class="number">.3</span>],</span><br><span class="line">               **blobs_params)[<span class="number">0</span>],</span><br><span class="line">    <span class="number">4.</span> * (make_moons(n_samples=n_samples, noise=<span class="number">.05</span>, random_state=<span class="number">0</span>)[<span class="number">0</span>] -</span><br><span class="line">          np.array([<span class="number">0.5</span>, <span class="number">0.25</span>])),</span><br><span class="line">    <span class="number">14.</span> * (np.random.RandomState(<span class="number">42</span>).rand(n_samples, <span class="number">2</span>) - <span class="number">0.5</span>)]</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Compare given classifiers under given settings</span></span><br><span class="line">xx, yy = np.meshgrid(np.linspace(-<span class="number">7</span>, <span class="number">7</span>, <span class="number">150</span>),</span><br><span class="line">                     np.linspace(-<span class="number">7</span>, <span class="number">7</span>, <span class="number">150</span>))</span><br><span class="line">​</span><br><span class="line">plt.figure(figsize=(<span class="built_in">len</span>(anomaly_algorithms) * <span class="number">2</span> + <span class="number">3</span>, <span class="number">12.5</span>))</span><br><span class="line">plt.subplots_adjust(left=<span class="number">.02</span>, right=<span class="number">.98</span>, bottom=<span class="number">.001</span>, top=<span class="number">.96</span>, wspace=<span class="number">.05</span>,</span><br><span class="line">                    hspace=<span class="number">.01</span>)</span><br><span class="line"></span><br><span class="line">plot_num = <span class="number">1</span></span><br><span class="line">rng = np.random.RandomState(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i_dataset, X <span class="keyword">in</span> <span class="built_in">enumerate</span>(datasets):</span><br><span class="line">    <span class="comment"># Add outliers</span></span><br><span class="line">    X = np.concatenate([X, rng.uniform(low=-<span class="number">6</span>, high=<span class="number">6</span>,</span><br><span class="line">                       size=(n_outliers, <span class="number">2</span>))], axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> name, algorithm <span class="keyword">in</span> anomaly_algorithms:</span><br><span class="line">        t0 = time.time()</span><br><span class="line">        algorithm.fit(X)</span><br><span class="line">        t1 = time.time()</span><br><span class="line">        plt.subplot(<span class="built_in">len</span>(datasets), <span class="built_in">len</span>(anomaly_algorithms), plot_num)</span><br><span class="line">        <span class="keyword">if</span> i_dataset == <span class="number">0</span>:</span><br><span class="line">            plt.title(name, size=<span class="number">18</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># fit the data and tag outliers</span></span><br><span class="line">        <span class="keyword">if</span> name == <span class="string">&quot;Local Outlier Factor&quot;</span>:</span><br><span class="line">            y_pred = algorithm.fit_predict(X)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y_pred = algorithm.fit(X).predict(X)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># plot the levels lines and the points</span></span><br><span class="line">        <span class="keyword">if</span> name != <span class="string">&quot;Local Outlier Factor&quot;</span>:  <span class="comment"># LOF does not implement predict</span></span><br><span class="line">            Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">            Z = Z.reshape(xx.shape)</span><br><span class="line">            plt.contour(xx, yy, Z, levels=[<span class="number">0</span>], linewidths=<span class="number">2</span>, colors=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        colors = np.array([<span class="string">&#x27;#377eb8&#x27;</span>, <span class="string">&#x27;#ff7f00&#x27;</span>])</span><br><span class="line">        plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], s=<span class="number">10</span>, color=colors[(y_pred + <span class="number">1</span>) // <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        plt.xlim(-<span class="number">7</span>, <span class="number">7</span>)</span><br><span class="line">        plt.ylim(-<span class="number">7</span>, <span class="number">7</span>)</span><br><span class="line">        plt.xticks(())</span><br><span class="line">        plt.yticks(())</span><br><span class="line">        plt.text(<span class="number">.99</span>, <span class="number">.01</span>, (<span class="string">&#x27;%.2fs&#x27;</span> % (t1 - t0)).lstrip(<span class="string">&#x27;0&#x27;</span>),</span><br><span class="line">                 transform=plt.gca().transAxes, size=<span class="number">15</span>,</span><br><span class="line">                 horizontalalignment=<span class="string">&#x27;right&#x27;</span>)</span><br><span class="line">        plot_num += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>对于每个数据集，产生15%的样本作为随机均匀噪声。这个比例是给予OneClassSVM的nu参数和其他离群点检测算法的污染参数的值。由于局部离群因子(LOF)用于离群值检测时没有对新数据应用的预测方法，因此除了局部离群值因子(LOF)外，inliers和离群值之间的决策边界以黑色显示。</p>
<p>sklearn.svm。一个已知的eclasssvm对异常值很敏感，因此在异常值检测方面表现不太好。该估计器最适合在训练集没有异常值的情况下进行新颖性检测。也就是说，在高维的离群点检测，或者在不对嵌入数据的分布做任何假设的情况下，一个类支持向量机可能在这些情况下给出有用的结果，这取决于它的超参数的值。</p>
<p>sklearn.covariance。椭圆包络假设数据是高斯分布，并学习一个椭圆。因此，当数据不是单峰时，它就会退化。但是请注意，这个估计器对异常值是稳健的。</p>
<p>sklearn.ensemble。IsolationForest sklearn.neighbors。LocalOutlierFactor对于多模态数据集似乎表现得相当好。sklearn的优势。第三个数据集的局部离群因子超过其他估计显示，其中两种模式有不同的密度。这种优势是由LOF的局域性来解释的，即它只比较一个样本的异常分数与其相邻样本的异常分数。</p>
<p>最后，对于最后一个数据集，很难说一个样本比另一个样本更反常，因为它们是均匀分布在超立方体中。除了sklearn。svm。有一点过度拟合的支持向量机，所有的估计器都对这种情况给出了合适的解决方案。在这种情况下，明智的做法是更密切地观察样本的异常分数，因为一个好的估计器应该给所有样本分配相似的分数。</p>
<p><strong>一个新奇点检测例子</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.font_manager</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">​</span><br><span class="line">xx, yy = np.meshgrid(np.linspace(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">500</span>), np.linspace(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">500</span>))</span><br><span class="line"><span class="comment"># Generate train data</span></span><br><span class="line">X = <span class="number">0.3</span> * np.random.randn(<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">X_train = np.r_[X + <span class="number">2</span>, X - <span class="number">2</span>]</span><br><span class="line"><span class="comment"># Generate some regular novel observations</span></span><br><span class="line">X = <span class="number">0.3</span> * np.random.randn(<span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line">X_test = np.r_[X + <span class="number">2</span>, X - <span class="number">2</span>]</span><br><span class="line"><span class="comment"># Generate some abnormal novel observations</span></span><br><span class="line">X_outliers = np.random.uniform(low=-<span class="number">4</span>, high=<span class="number">4</span>, size=(<span class="number">20</span>, <span class="number">2</span>))</span><br><span class="line">​</span><br><span class="line"><span class="comment"># fit the model</span></span><br><span class="line">clf = svm.OneClassSVM(nu=<span class="number">0.1</span>, kernel=<span class="string">&quot;rbf&quot;</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line">clf.fit(X_train)</span><br><span class="line">y_pred_train = clf.predict(X_train)</span><br><span class="line">y_pred_test = clf.predict(X_test)</span><br><span class="line">y_pred_outliers = clf.predict(X_outliers)</span><br><span class="line">n_error_train = y_pred_train[y_pred_train == -<span class="number">1</span>].size</span><br><span class="line">n_error_test = y_pred_test[y_pred_test == -<span class="number">1</span>].size</span><br><span class="line">n_error_outliers = y_pred_outliers[y_pred_outliers == <span class="number">1</span>].size</span><br><span class="line">​</span><br><span class="line"><span class="comment"># plot the line, the points, and the nearest vectors to the plane</span></span><br><span class="line">Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">Z = Z.reshape(xx.shape)</span><br><span class="line">​</span><br><span class="line">plt.title(<span class="string">&quot;Novelty Detection&quot;</span>)</span><br><span class="line">plt.contourf(xx, yy, Z, levels=np.linspace(Z.<span class="built_in">min</span>(), <span class="number">0</span>, <span class="number">7</span>), cmap=plt.cm.PuBu)</span><br><span class="line">a = plt.contour(xx, yy, Z, levels=[<span class="number">0</span>], linewidths=<span class="number">2</span>, colors=<span class="string">&#x27;darkred&#x27;</span>)</span><br><span class="line">plt.contourf(xx, yy, Z, levels=[<span class="number">0</span>, Z.<span class="built_in">max</span>()], colors=<span class="string">&#x27;palevioletred&#x27;</span>)</span><br><span class="line">​</span><br><span class="line">s = <span class="number">40</span></span><br><span class="line">b1 = plt.scatter(X_train[:, <span class="number">0</span>], X_train[:, <span class="number">1</span>], c=<span class="string">&#x27;white&#x27;</span>, s=s, edgecolors=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">b2 = plt.scatter(X_test[:, <span class="number">0</span>], X_test[:, <span class="number">1</span>], c=<span class="string">&#x27;blueviolet&#x27;</span>, s=s,</span><br><span class="line">                 edgecolors=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">c = plt.scatter(X_outliers[:, <span class="number">0</span>], X_outliers[:, <span class="number">1</span>], c=<span class="string">&#x27;gold&#x27;</span>, s=s,</span><br><span class="line">                edgecolors=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.axis(<span class="string">&#x27;tight&#x27;</span>)</span><br><span class="line">plt.xlim((-<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">plt.ylim((-<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">plt.legend([a.collections[<span class="number">0</span>], b1, b2, c],</span><br><span class="line">           [<span class="string">&quot;learned frontier&quot;</span>, <span class="string">&quot;training observations&quot;</span>,</span><br><span class="line">            <span class="string">&quot;new regular observations&quot;</span>, <span class="string">&quot;new abnormal observations&quot;</span>],</span><br><span class="line">           loc=<span class="string">&quot;upper left&quot;</span>,</span><br><span class="line">           prop=matplotlib.font_manager.FontProperties(size=<span class="number">11</span>))</span><br><span class="line">plt.xlabel(</span><br><span class="line">    <span class="string">&quot;error train: %d/200 ; errors novel regular: %d/40 ; &quot;</span></span><br><span class="line">    <span class="string">&quot;errors novel abnormal: %d/40&quot;</span></span><br><span class="line">    % (n_error_train, n_error_test, n_error_outliers))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>效果</p>
<p>1、考虑异常检测模型和分类模型，结果是<strong>分类模型比异常检测模型效果好</strong>；</p>
<p>2、分类模型中的<strong>层次聚类效果又比k-means聚类效果好</strong></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://genewlan.github.io">ZhangLei</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://genewlan.github.io/2023/07/31/outlier-detection/">http://genewlan.github.io/2023/07/31/outlier-detection/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://genewlan.github.io" target="_blank">GeneWlan</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Modeling/">Modeling</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/08/31/%E9%80%9A%E4%BF%97%E7%90%86%E8%A7%A3LDA%E4%B8%BB%E9%A2%98%E6%A8%A1/" title="通俗理解LDA主题模"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">通俗理解LDA主题模</div></div></a></div><div class="next-post pull-right"><a href="/2023/07/31/Matrix/" title="Nature of Matrix"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Nature of Matrix</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/05/16/how-to-modeling/" title="如何避免建模中的陷阱"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-16</div><div class="title">如何避免建模中的陷阱</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ZhangLei</div><div class="author-info__description">change or die!</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">18</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/genewlan/" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:xiaolobglee@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">单变量和多变量异常值</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">单变量异常值检测</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%88%86%E5%B8%83-%E6%A0%87%E5%87%86%E5%B7%AE%E6%B3%95"><span class="toc-number">2.1.</span> <span class="toc-text">基于分布–标准差法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%88%86%E5%B8%83-%E5%9B%9B%E5%88%86%E4%BD%8D%E8%B7%9D%E6%B3%95"><span class="toc-number">2.2.</span> <span class="toc-text">基于分布–四分位距法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%88%86%E5%B8%83-z-score"><span class="toc-number">2.3.</span> <span class="toc-text">基于分布–Z-score</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%88%86%E5%B8%83-grubbs%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C"><span class="toc-number">2.4.</span> <span class="toc-text">基于分布–Grubbs假设检验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E8%B7%9D%E7%A6%BB%E7%9A%84%E6%96%B9%E6%B3%95-knn"><span class="toc-number">2.5.</span> <span class="toc-text">基于距离的方法-KNN</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">多变量异常值检测</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A4%E7%AB%8B%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95-isolation-forest"><span class="toc-number">3.1.</span> <span class="toc-text">孤立森林算法-Isolation Forest</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E7%A9%BA%E9%97%B4%E5%AF%86%E5%BA%A6%E7%9A%84%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95-dbscan"><span class="toc-number">3.2.</span> <span class="toc-text">基于空间密度的聚类算法-DBSCAN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%AF%86%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95-%E5%B1%80%E9%83%A8%E5%BC%82%E5%B8%B8%E5%9B%A0%E5%AD%90%E7%AE%97%E6%B3%95-lof"><span class="toc-number">3.3.</span> <span class="toc-text">基于密度的方法–局部异常因子算法-LOF</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%AF%86%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95-connectivity-based-outlier-factor-cof"><span class="toc-number">3.4.</span> <span class="toc-text">基于密度的方法–Connectivity-Based Outlier Factor (COF)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%AF%86%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95-stochastic-outlier-selection-sos"><span class="toc-number">3.5.</span> <span class="toc-text">基于密度的方法–Stochastic Outlier Selection (SOS)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%88%86%E7%B1%BB%E7%9A%84%E6%96%B9%E6%B3%95-oneclasssvm"><span class="toc-number">3.6.</span> <span class="toc-text">基于分类的方法–OneClassSVM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%88%86%E7%B1%BB%E7%9A%84%E6%96%B9%E6%B3%95-elliptic-envelope"><span class="toc-number">3.7.</span> <span class="toc-text">基于分类的方法–Elliptic Envelope</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E9%99%8D%E7%BB%B4%E7%9A%84%E6%96%B9%E6%B3%95-principal-component-analysis-pca"><span class="toc-number">3.8.</span> <span class="toc-text">基于降维的方法–Principal Component Analysis (PCA)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E9%99%8D%E7%BB%B4%E7%9A%84%E6%96%B9%E6%B3%95-autoencoder"><span class="toc-number">3.9.</span> <span class="toc-text">基于降维的方法–AutoEncoder</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">离群检测与新奇检测</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/11/30/Clustering/" title="Clustering">Clustering</a><time datetime="2023-11-30T13:03:41.000Z" title="发表于 2023-11-30 21:03:41">2023-11-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/11/30/Transformer/" title="Transformer">Transformer</a><time datetime="2023-11-30T12:44:09.000Z" title="发表于 2023-11-30 20:44:09">2023-11-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/10/31/ggplot-picture/" title="ggplot_picture_bioinformation">ggplot_picture_bioinformation</a><time datetime="2023-10-31T09:08:06.000Z" title="发表于 2023-10-31 17:08:06">2023-10-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/10/31/picture/" title="50_ggplot_picture">50_ggplot_picture</a><time datetime="2023-10-31T08:48:54.000Z" title="发表于 2023-10-31 16:48:54">2023-10-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/09/30/Neural-Network-Frame/" title="Neural-Network-Frame">Neural-Network-Frame</a><time datetime="2023-09-30T10:55:16.000Z" title="发表于 2023-09-30 18:55:16">2023-09-30</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 By ZhangLei</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'J0s1l0MeDfMbgw4y4awgy2jX-MdYXbMMI',
      appKey: '3vZoaKxqWQYlKbXXdXuSxBsT',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div><!-- hexo injector body_end start --><div id="background-effect"></div><script src="https://cdn.jsdelivr.net/npm/three@0.121.1/build/three.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanta/dist/vanta.birds.min.js"></script><script>VANTA.BIRDS({"el":"#background-effect","mouseControls":true,"touchControls":true,"gyroControls":false,"minHeight":200,"minWidth":200,"scale":1,"scaleMobile":1})</script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"react":{"opacityDefault":1,"opacityOnHover":1},"log":false,"tagMode":false});</script></body></html>