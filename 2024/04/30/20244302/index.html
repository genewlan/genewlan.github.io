<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>20244302_Validation_and_Model_Selection | GeneWlan</title><meta name="author" content="ZhangLei"><meta name="copyright" content="ZhangLei"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="[TOC] Validation and Model Selection Credits: Forked from PyCon 2015 Scikit-learn Tutorial by Jake VanderPlas In this section, we’ll look at model evaluation and the tuning of hyperparameters, which a">
<meta property="og:type" content="article">
<meta property="og:title" content="20244302_Validation_and_Model_Selection">
<meta property="og:url" content="http://genewlan.github.io/2024/04/30/20244302/index.html">
<meta property="og:site_name" content="GeneWlan">
<meta property="og:description" content="[TOC] Validation and Model Selection Credits: Forked from PyCon 2015 Scikit-learn Tutorial by Jake VanderPlas In this section, we’ll look at model evaluation and the tuning of hyperparameters, which a">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2024-04-30T10:16:27.000Z">
<meta property="article:modified_time" content="2024-02-29T11:18:46.297Z">
<meta property="article:author" content="ZhangLei">
<meta property="article:tag" content="Validation Model Selection">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://genewlan.github.io/2024/04/30/20244302/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '20244302_Validation_and_Model_Selection',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-02-29 19:18:46'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
    <style>
      #background-effect {
        position: fixed !important;
        top: 0px;
        left: 0px;
        z-index: -1;
        width: 100%;
        height: 100%
      }
    </style>
  <!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="GeneWlan" type="application/atom+xml">
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">23</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2023/03/21/ZVUmQFknE2JosXT.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="GeneWlan"><span class="site-name">GeneWlan</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">20244302_Validation_and_Model_Selection</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-04-30T10:16:27.000Z" title="发表于 2024-04-30 18:16:27">2024-04-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-02-29T11:18:46.297Z" title="更新于 2024-02-29 19:18:46">2024-02-29</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="20244302_Validation_and_Model_Selection"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>[TOC]</p>
<h1>Validation and Model Selection</h1>
<p>Credits: Forked from <a target="_blank" rel="noopener" href="https://github.com/jakevdp/sklearn_pycon2015">PyCon 2015 Scikit-learn Tutorial</a> by Jake VanderPlas</p>
<p>In this section, we’ll look at <em>model evaluation</em> and the tuning of <em>hyperparameters</em>, which are parameters that define the model.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import print_function, division</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns; sns.set()</span><br></pre></td></tr></table></figure>
<h2 id="validating-models">Validating Models</h2>
<p>One of the most important pieces of machine learning is <strong>model validation</strong>: that is, checking how well your model fits a given dataset. But there are some pitfalls you need to watch out for.</p>
<p>Consider the digits example we’ve been looking at previously. How might we check how well our model fits the data?</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_digits</span><br><span class="line">digits = load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Let’s fit a K-neighbors classifier</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=1)</span><br><span class="line">knn.fit(X, y)</span><br><span class="line">Now we&#x27;ll use this classifier to *predict* labels for the data</span><br><span class="line">y_pred = knn.predict(X)</span><br><span class="line">Finally, we can check how well our prediction did:</span><br><span class="line">print(&quot;&#123;0&#125; / &#123;1&#125; correct&quot;.format(np.sum(y == y_pred), len(y)))</span><br></pre></td></tr></table></figure>
<p>It seems we have a perfect classifier!</p>
<p><strong>Question: what’s wrong with this?</strong></p>
<h2 id="validation-sets">Validation Sets</h2>
<p>Above we made the mistake of testing our data on the same set of data that was used for training. <strong>This is not generally a good idea</strong>. If we optimize our estimator this way, we will tend to <strong>over-fit</strong> the data: that is, we learn the noise.</p>
<p>A better way to test a model is to use a hold-out set which doesn’t enter the training. We’ve seen this before using scikit-learn’s train/test split utility:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cross_validation import train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y)</span><br><span class="line">X_train.shape, X_test.shape</span><br></pre></td></tr></table></figure>
<p>Now we train on the training data, and validate on the test data:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">knn = KNeighborsClassifier(n_neighbors=1)</span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line">print(&quot;&#123;0&#125; / &#123;1&#125; correct&quot;.format(np.sum(y_test == y_pred), len(y_test)))</span><br></pre></td></tr></table></figure>
<p>This gives us a more reliable estimate of how our model is doing.</p>
<p>The metric we’re using here, comparing the number of matches to the total number of samples, is known as the <strong>accuracy score</strong>, and can be computed using the following routine:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import accuracy_score</span><br><span class="line">accuracy_score(y_test, y_pred)</span><br></pre></td></tr></table></figure>
<p>This can also be computed directly from the <code>model.score</code> method:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knn.score(X_test, y_test)</span><br></pre></td></tr></table></figure>
<p>Using this, we can ask how this changes as we change the model parameters, in this case the number of neighbors:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for n_neighbors in [1, 5, 10, 20, 30]:</span><br><span class="line">    knn = KNeighborsClassifier(n_neighbors)</span><br><span class="line">    knn.fit(X_train, y_train)</span><br><span class="line">    print(n_neighbors, knn.score(X_test, y_test))</span><br></pre></td></tr></table></figure>
<p>We see that in this case, a small number of neighbors seems to be the best option.</p>
<h2 id="cross-validation">Cross-Validation</h2>
<p>One problem with validation sets is that you “lose” some of the data. Above, we’ve only used 3/4 of the data for the training, and used 1/4 for the validation. Another option is to use <strong>2-fold cross-validation</strong>, where we split the sample in half and perform the validation twice:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X1, X2, y1, y2 = train_test_split(X, y, test_size=0.5, random_state=0)</span><br><span class="line">X1.shape, X2.shape</span><br><span class="line">print(KNeighborsClassifier(1).fit(X2, y2).score(X1, y1))</span><br><span class="line">print(KNeighborsClassifier(1).fit(X1, y1).score(X2, y2))</span><br></pre></td></tr></table></figure>
<p>Thus a two-fold cross-validation gives us two estimates of the score for that parameter.</p>
<p>Because this is a bit of a pain to do by hand, scikit-learn has a utility routine to help:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cross_validation import cross_val_score</span><br><span class="line">cv = cross_val_score(KNeighborsClassifier(1), X, y, cv=10)</span><br><span class="line">cv.mean()</span><br></pre></td></tr></table></figure>
<h3 id="k-fold-cross-validation">K-fold Cross-Validation</h3>
<p>Here we’ve used 2-fold cross-validation. This is just one specialization of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span></span></span></span>-fold cross-validation, where we split the data into <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span></span></span></span> chunks and perform <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span></span></span></span> fits, where each chunk gets a turn as the validation set.<br>
We can do this by changing the <code>cv</code> parameter above. Let’s do 10-fold cross-validation:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_val_score(KNeighborsClassifier(1), X, y, cv=10)</span><br></pre></td></tr></table></figure>
<p>This gives us an even better idea of how well our model is doing.</p>
<h2 id="overfitting-underfitting-and-model-selection">Overfitting, Underfitting and Model Selection</h2>
<p>Now that we’ve gone over the basics of validation, and cross-validation, it’s time to go into even more depth regarding model selection.</p>
<p>The issues associated with validation and cross-validation are some of the most important<br>
aspects of the practice of machine learning.  Selecting the optimal model for your data is vital, and is a piece of the problem that is not often appreciated by machine learning practitioners.</p>
<p>Of core importance is the following question:</p>
<p><strong>If our estimator is underperforming, how should we move forward?</strong></p>
<ul>
<li>Use simpler or more complicated model?</li>
<li>Add more features to each observed data point?</li>
<li>Add more training samples?</li>
</ul>
<p>The answer is often counter-intuitive.  In particular, <strong>Sometimes using a more complicated model will give <em>worse</em> results.</strong>  Also, <strong>Sometimes adding training data will not improve your results.</strong>  The ability to determine what steps will improve your model is what separates the successful machine<br>
learning practitioners from the unsuccessful.</p>
<h3 id="illustration-of-the-bias-variance-tradeoff">Illustration of the Bias-Variance Tradeoff</h3>
<p>For this section, we’ll work with a simple 1D regression problem.  This will help us to<br>
easily visualize the data and the model, and the results generalize easily to  higher-dimensional<br>
datasets.  We’ll explore a simple <strong>linear regression</strong> problem.<br>
This can be accomplished within scikit-learn with the <code>sklearn.linear_model</code> module.</p>
<p>We’ll create a simple nonlinear function that we’d like to fit</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def test_func(x, err=0.5):</span><br><span class="line">    y = 10 - 1. / (x + 0.1)</span><br><span class="line">    if err &gt; 0:</span><br><span class="line">        y = np.random.normal(y, err)</span><br><span class="line">    return y</span><br></pre></td></tr></table></figure>
<p>Now let’s create a realization of this dataset:</p>
<pre><code>def make_data(N=40, error=1.0, random_seed=1):
#randomly sample the data
    np.random.seed(1)
    X = np.random.random(N)[:, np.newaxis]
    y = test_func(X.ravel(), error)
return X, y
X, y = make_data(40, error=1)
plt.scatter(X.ravel(), y);
</code></pre>
<p>Now say we want to perform a regression on this data.  Let’s use the built-in linear regression function to compute a fit:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X_test = np.linspace(-0.1, 1.1, 500)[:, None]</span><br><span class="line"></span><br><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">from sklearn.metrics import mean_squared_error</span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(X, y)</span><br><span class="line">y_test = model.predict(X_test)</span><br><span class="line"></span><br><span class="line">plt.scatter(X.ravel(), y)</span><br><span class="line">plt.plot(X_test.ravel(), y_test)</span><br><span class="line">plt.title(&quot;mean squared error: &#123;0:.3g&#125;&quot;.format(mean_squared_error(model.predict(X), y)));</span><br></pre></td></tr></table></figure>
<p>We have fit a straight line to the data, but clearly this model is not a good choice.  We say that this model is <strong>biased</strong>, or that it <strong>under-fits</strong> the data.</p>
<p>Let’s try to improve this by creating a more complicated model.  We can do this by adding degrees of freedom, and computing a polynomial regression over the inputs. Scikit-learn makes this easy with the <code>PolynomialFeatures</code> preprocessor, which can be pipelined with a linear regression.</p>
<p>Let’s make a convenience routine to do this:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">from sklearn.pipeline import make_pipeline</span><br><span class="line"></span><br><span class="line">def PolynomialRegression(degree=2, **kwargs):</span><br><span class="line">    return make_pipeline(PolynomialFeatures(degree),</span><br><span class="line">                         LinearRegression(**kwargs))</span><br><span class="line">Now we&#x27;ll use this to fit a quadratic curve to the data.</span><br><span class="line">model = PolynomialRegression(2)</span><br><span class="line">model.fit(X, y)</span><br><span class="line">y_test = model.predict(X_test)</span><br><span class="line"></span><br><span class="line">plt.scatter(X.ravel(), y)</span><br><span class="line">plt.plot(X_test.ravel(), y_test)</span><br><span class="line">plt.title(&quot;mean squared error: &#123;0:.3g&#125;&quot;.format(mean_squared_error(model.predict(X), y)));</span><br></pre></td></tr></table></figure>
<p>This reduces the mean squared error, and makes a much better fit.  What happens if we use an even higher-degree polynomial?</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model = PolynomialRegression(30)</span><br><span class="line">model.fit(X, y)</span><br><span class="line">y_test = model.predict(X_test)</span><br><span class="line"></span><br><span class="line">plt.scatter(X.ravel(), y)</span><br><span class="line">plt.plot(X_test.ravel(), y_test)</span><br><span class="line">plt.title(&quot;mean squared error: &#123;0:.3g&#125;&quot;.format(mean_squared_error(model.predict(X), y)))</span><br><span class="line">plt.ylim(-4, 14);</span><br></pre></td></tr></table></figure>
<p>When we increase the degree to this extent, it’s clear that the resulting fit is no longer reflecting the true underlying distribution, but is more sensitive to the noise in the training data. For this reason, we call it a <strong>high-variance model</strong>, and we say that it <strong>over-fits</strong> the data.<br>
Just for fun, let’s use IPython’s interact capability (only in IPython 2.0+) to explore this interactively:</p>
<pre><code>from IPython.html.widgets import interact

def plot_fit(degree=1, Npts=50):
    X, y = make_data(Npts, error=1)
    X_test = np.linspace(-0.1, 1.1, 500)[:, None]
model = PolynomialRegression(degree=degree)
model.fit(X, y)
y_test = model.predict(X_test)

plt.scatter(X.ravel(), y)
plt.plot(X_test.ravel(), y_test)
plt.ylim(-4, 14)
plt.title(&quot;mean squared error: &#123;0:.2f&#125;&quot;.format(mean_squared_error(model.predict(X), y)))
interact(plot_fit, degree=[1, 30], Npts=[2, 100]);
</code></pre>
<h3 id="detecting-over-fitting-with-validation-curves">Detecting Over-fitting with Validation Curves</h3>
<p>Clearly, computing the error on the training data is not enough (we saw this previously). As above, we can use <strong>cross-validation</strong> to get a better handle on how the model fit is working.</p>
<p>Let’s do this here, again using the <code>validation_curve</code> utility. To make things more clear, we’ll use a slightly larger dataset:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X, y = make_data(120, error=1.0)</span><br><span class="line">plt.scatter(X, y);</span><br><span class="line">from sklearn.learning_curve import validation_curve</span><br><span class="line"></span><br><span class="line">def rms_error(model, X, y):</span><br><span class="line">    y_pred = model.predict(X)</span><br><span class="line">    return np.sqrt(np.mean((y - y_pred) ** 2))</span><br><span class="line"></span><br><span class="line">degree = np.arange(0, 18)</span><br><span class="line">val_train, val_test = validation_curve(PolynomialRegression(), X, y,</span><br><span class="line">                                       &#x27;polynomialfeatures__degree&#x27;, degree, cv=7,</span><br><span class="line">                                       scoring=rms_error)</span><br></pre></td></tr></table></figure>
<p>Now let’s plot the validation curves:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def plot_with_err(x, data, **kwargs):</span><br><span class="line">    mu, std = data.mean(1), data.std(1)</span><br><span class="line">    lines = plt.plot(x, mu, &#x27;-&#x27;, **kwargs)</span><br><span class="line">    plt.fill_between(x, mu - std, mu + std, edgecolor=&#x27;none&#x27;,</span><br><span class="line">                     facecolor=lines[0].get_color(), alpha=0.2)</span><br><span class="line"></span><br><span class="line">plot_with_err(degree, val_train, label=&#x27;training scores&#x27;)</span><br><span class="line">plot_with_err(degree, val_test, label=&#x27;validation scores&#x27;)</span><br><span class="line">plt.xlabel(&#x27;degree&#x27;); plt.ylabel(&#x27;rms error&#x27;)</span><br><span class="line">plt.legend();</span><br></pre></td></tr></table></figure>
<p>Notice the trend here, which is common for this type of plot.</p>
<p>Notice the trend here, which is common for this type of plot.</p>
<ol>
<li>
<p>For a small model complexity, the training error and validation error are very similar. This indicates that the model is <strong>under-fitting</strong> the data: it doesn’t have enough complexity to represent the data. Another way of putting it is that this is a <strong>high-bias</strong> model.</p>
</li>
<li>
<p>As the model complexity grows, the training and validation scores diverge. This indicates that the model is <strong>over-fitting</strong> the data: it has so much flexibility, that it fits the noise rather than the underlying trend. Another way of putting it is that this is a <strong>high-variance</strong> model.</p>
</li>
<li>
<p>Note that the training score (nearly) always improves with model complexity. This is because a more complicated model can fit the noise better, so the model improves. The validation data generally has a sweet spot, which here is around 5 terms.</p>
</li>
</ol>
<p>Here’s our best-fit model according to the cross-validation:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = PolynomialRegression(4).fit(X, y)</span><br><span class="line">plt.scatter(X, y)</span><br><span class="line">plt.plot(X_test, model.predict(X_test));</span><br></pre></td></tr></table></figure>
<h3 id="detecting-data-sufficiency-with-learning-curves">Detecting Data Sufficiency with Learning Curves</h3>
<p>As you might guess, the exact turning-point of the tradeoff between bias and variance is highly dependent on the number of training points used.  Here we’ll illustrate the use of <em>learning curves</em>, which display this property.</p>
<p>The idea is to plot the mean-squared-error for the training and test set as a function of <em>Number of Training Points</em><br>
from sklearn.learning_curve import learning_curve</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def plot_learning_curve(degree=3):</span><br><span class="line">    train_sizes = np.linspace(0.05, 1, 20)</span><br><span class="line">    N_train, val_train, val_test = learning_curve(PolynomialRegression(degree),</span><br><span class="line">                                                  X, y, train_sizes, cv=5,</span><br><span class="line">                                                  scoring=rms_error)</span><br><span class="line">    plot_with_err(N_train, val_train, label=&#x27;training scores&#x27;)</span><br><span class="line">    plot_with_err(N_train, val_test, label=&#x27;validation scores&#x27;)</span><br><span class="line">    plt.xlabel(&#x27;Training Set Size&#x27;); plt.ylabel(&#x27;rms error&#x27;)</span><br><span class="line">    plt.ylim(0, 3)</span><br><span class="line">    plt.xlim(5, 80)</span><br><span class="line">    plt.legend()</span><br></pre></td></tr></table></figure>
<p>Let’s see what the learning curves look like for a linear model:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_learning_curve(1)</span><br></pre></td></tr></table></figure>
<p>This shows a typical learning curve: for very few training points, there is a large separation between the training and test error, which indicates <strong>over-fitting</strong>.  Given the same model, for a large number of training points, the training and testing errors converge, which indicates potential <strong>under-fitting</strong>.</p>
<p>As you add more data points, the training error will never increase, and the testing error will never decrease (why do you think this is?)</p>
<p>It is easy to see that, in this plot, if you’d like to reduce the MSE down to the nominal value of 1.0 (which is the magnitude of the scatter we put in when constructing the data), then adding more samples will <em>never</em> get you there.  For <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">d=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>, the two curves have converged and cannot move lower. What about for a larger value of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span></span></span></span>?</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_learning_curve(3)</span><br></pre></td></tr></table></figure>
<p>Here we see that by adding more model complexity, we’ve managed to lower the level of convergence to an rms error of 1.0!</p>
<p>What if we get even more complex?<br>
plot_learning_curve(10)<br>
For an even more complex model, we still converge, but the convergence only happens for <em>large</em> amounts of training data.</p>
<p>So we see the following:</p>
<ul>
<li>you can <strong>cause the lines to converge</strong> by adding more points or by simplifying the model.</li>
<li>you can <strong>bring the convergence error down</strong> only by increasing the complexity of the model.</li>
</ul>
<p>Thus these curves can give you hints about how you might improve a sub-optimal model. If the curves are already close together, you need more model complexity. If the curves are far apart, you might also improve the model by adding more data.</p>
<p>To make this more concrete, imagine some telescope data in which the results are not robust enough.  You must think about whether to spend your valuable telescope time observing <em>more objects</em> to get a larger training set, or <em>more attributes of each object</em> in order to improve the model.  The answer to this question has real consequences, and can be addressed using these metrics.</p>
<h2 id="summary">Summary</h2>
<p>We’ve gone over several useful tools for model validation</p>
<ul>
<li>The <strong>Training Score</strong> shows how well a model fits the data it was trained on. This is not a good indication of model effectiveness</li>
<li>The <strong>Validation Score</strong> shows how well a model fits hold-out data. The most effective method is some form of cross-validation, where multiple hold-out sets are used.</li>
<li><strong>Validation Curves</strong> are a plot of validation score and training score as a function of <strong>model complexity</strong>:
<ul>
<li>when the two curves are close, it indicates <em>underfitting</em></li>
<li>when the two curves are separated, it indicates <em>overfitting</em></li>
<li>the “sweet spot” is in the middle</li>
</ul>
</li>
<li><strong>Learning Curves</strong> are a plot of the validation score and training score as a function of <strong>Number of training samples</strong>
<ul>
<li>when the curves are close, it indicates <em>underfitting</em>, and adding more data will not generally improve the estimator.</li>
<li>when the curves are far apart, it indicates <em>overfitting</em>, and adding more data may increase the effectiveness of the model.</li>
</ul>
</li>
</ul>
<p>These tools are powerful means of evaluating your model on your data.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://genewlan.github.io">ZhangLei</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://genewlan.github.io/2024/04/30/20244302/">http://genewlan.github.io/2024/04/30/20244302/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://genewlan.github.io" target="_blank">GeneWlan</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Validation-Model-Selection/">Validation Model Selection</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/04/30/20244301/" title="20244301_random forest"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">20244301_random forest</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ZhangLei</div><div class="author-info__description">change or die!</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">23</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/genewlan/" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:xiaolobglee@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Validation and Model Selection</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#validating-models"><span class="toc-number">1.1.</span> <span class="toc-text">Validating Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#validation-sets"><span class="toc-number">1.2.</span> <span class="toc-text">Validation Sets</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cross-validation"><span class="toc-number">1.3.</span> <span class="toc-text">Cross-Validation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#k-fold-cross-validation"><span class="toc-number">1.3.1.</span> <span class="toc-text">K-fold Cross-Validation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#overfitting-underfitting-and-model-selection"><span class="toc-number">1.4.</span> <span class="toc-text">Overfitting, Underfitting and Model Selection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#illustration-of-the-bias-variance-tradeoff"><span class="toc-number">1.4.1.</span> <span class="toc-text">Illustration of the Bias-Variance Tradeoff</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#detecting-over-fitting-with-validation-curves"><span class="toc-number">1.4.2.</span> <span class="toc-text">Detecting Over-fitting with Validation Curves</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#detecting-data-sufficiency-with-learning-curves"><span class="toc-number">1.4.3.</span> <span class="toc-text">Detecting Data Sufficiency with Learning Curves</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#summary"><span class="toc-number">1.5.</span> <span class="toc-text">Summary</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/04/30/20244302/" title="20244302_Validation_and_Model_Selection">20244302_Validation_and_Model_Selection</a><time datetime="2024-04-30T10:16:27.000Z" title="发表于 2024-04-30 18:16:27">2024-04-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/04/30/20244301/" title="20244301_random forest">20244301_random forest</a><time datetime="2024-04-30T10:16:10.000Z" title="发表于 2024-04-30 18:16:10">2024-04-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/03/31/20243312/" title="20243312_PCA">20243312_PCA</a><time datetime="2024-03-31T10:04:12.000Z" title="发表于 2024-03-31 18:04:12">2024-03-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/03/31/20243311/" title="20243311_SVM">20243311_SVM</a><time datetime="2024-03-31T10:03:56.000Z" title="发表于 2024-03-31 18:03:56">2024-03-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/02/29/20242292/" title="20242292_k-means">20242292_k-means</a><time datetime="2024-02-29T09:43:33.000Z" title="发表于 2024-02-29 17:43:33">2024-02-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By ZhangLei</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'J0s1l0MeDfMbgw4y4awgy2jX-MdYXbMMI',
      appKey: '3vZoaKxqWQYlKbXXdXuSxBsT',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div><!-- hexo injector body_end start --><div id="background-effect"></div><script src="https://cdn.jsdelivr.net/npm/three@0.121.1/build/three.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanta/dist/vanta.birds.min.js"></script><script>VANTA.BIRDS({"el":"#background-effect","mouseControls":true,"touchControls":true,"gyroControls":false,"minHeight":200,"minWidth":200,"scale":1,"scaleMobile":1})</script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"react":{"opacityDefault":1,"opacityOnHover":1},"log":false,"tagMode":false});</script></body></html>